{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openai in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.54.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: torch in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリをインストール\n",
    "! pip install pandas\n",
    "! pip install openai faiss-cpu transformers\n",
    "! pip install faiss-cpu\n",
    "! pip install torch\n",
    "! pip install numpy\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain langchain-community langchain_text_splitters langchain_chroma\n",
    "! pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1. Load from the cloud on the Kaggle database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle is the famous platform which is offering machine learning compititions and opensource datasets\n",
    "\n",
    "Note that if you want to use Kaggle, you must have an account but one can easily log in using one's Google or Fakebook or Yahoo account\n",
    "\n",
    "We need to install Kaggle API (Application Programming Interface) first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Collecting certifi>=2023.7.22 (from kaggle)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from kaggle) (2.9.0)\n",
      "Collecting requests (from kaggle)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from kaggle)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting urllib3 (from kaggle)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting bleach (from kaggle)\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting webencodings (from bleach->kaggle)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->kaggle)\n",
      "  Downloading charset_normalizer-3.4.0-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->kaggle)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105797 sha256=806d80a5e7a85bff173dd323dd57722f7b423ad2895e93f82d03411925a4c855\n",
      "  Stored in directory: c:\\users\\wangs\\appdata\\local\\pip\\cache\\wheels\\2b\\af\\a9\\70bffa2773af622d2ebea9c8d407720b86e67bd40c465bf837\n",
      "Successfully built kaggle\n",
      "Installing collected packages: webencodings, text-unidecode, urllib3, tqdm, python-slugify, idna, charset-normalizer, certifi, bleach, requests, kaggle\n",
      "Successfully installed bleach-6.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 kaggle-1.6.17 python-slugify-8.0.4 requests-2.32.3 text-unidecode-1.3 tqdm-4.67.0 urllib3-2.2.3 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Kaggle home page in your browser, click on the icon in the upper right corner and select \"Account\" from the menu bar to go to the account management page. In the \"API\" section, select \"Create New API Token\" to download the kaggle.json file to your computer. **After uploading that json file to colab by clicking on the folder symbol in the leftmost tab**, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "指定したディレクトリにAPIファイルを入れてください:C:/Users/wangs/.kaggle\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/wangs/.kaggle\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "print(f\"指定したディレクトリにAPIファイルを入れてください:{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Page Token = CfDJ8GrUjsAKvhxNhvm67MmtKLl39PI19c56I5tY-a_zC4a4Hv2AGugR0W3XR5n4ElQhQ8XVOEzjSsXWZpCTF1zKo5Q\n",
      "    id  ref                                                       title                                          subtitle                                                                                                                                                                                                              author              \n",
      "------  --------------------------------------------------------  ---------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------  \n",
      "121027  metaresearch/llama-3.2                                    Llama 3.2                                      The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).                               Meta                \n",
      "145528  cohereforai/aya-expanse                                   Aya Expanse                                    Aya Expanse advances the state-of-the-art in multilingual capabilities to help bridge language gaps with AI.                                                                                                          CohereForAI         \n",
      "127417  abdullahmeda/qwen2.5                                      Qwen2.5                                                                                                                                                                                                                                                              Abdullah Meda       \n",
      "146350  dasbro/xgb_model                                          XGB_model                                                                                                                                                                                                                                                            dasbro              \n",
      "121954  google/gemma-2-2b-jpn-it                                  Gemma 2 2b JPN IT                              Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.                                                             Google              \n",
      "141350  ultralytics/yolo11                                        Ultralytics YOLO11                             Ultralytics YOLO 🚀 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.                                                                                 Ultralytics         \n",
      "  3301  google/gemma                                              Gemma                                          Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.                                                                                    Google              \n",
      "123481  takanashihumbert/qwen2.5                                  Qwen2.5                                                                                                                                                                                                                                                              Joseph              \n",
      "121030  metaresearch/llama-3.2-vision                             Llama 3.2 Vision                               The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out).  Meta                \n",
      "  3533  keras/gemma                                               Gemma                                          Keras implementation of the Gemma model. This Keras 3 implementation will run on JAX, TensorFlow and PyTorch.                                                                                                         Keras               \n",
      "  1902  mistral-ai/mistral                                        Mistral                                        Mistral AI team is proud to release Mistral, the most powerful language model for its size to date.                                                                                                                   Mistral AI          \n",
      "141013  ultralytics/yolov8                                        Ultralytics YOLOv8                             Ultralytics YOLO 🚀 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.                                                                                 Ultralytics         \n",
      "141044  huikang/qwen2.5                                           qwen2.5                                                                                                                                                                                                                                                              Tong Hui Kang       \n",
      " 91102  metaresearch/llama-3.1                                    Llama 3.1                                      The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).                        Meta                \n",
      " 78150  keras/gemma2                                              Gemma 2                                        Keras implementation of the Gemma 2 model. This Keras 3 implementation will run on JAX, TensorFlow and PyTorch.                                                                                                       Keras               \n",
      "128845  jonathanchan/baai                                         BAAI                                                                                                                                                                                                                                                                 Jonathan Chan       \n",
      "152800  ai-singapore/gemma2-9b-cpt-sea-lionv3-instruct            gemma2-9b-cpt-sea-lionv3-instruct              Gemma2 9B CPT SEA-LIONv3 Instruct                                                                                                                                                                                     AI Singapore        \n",
      " 76277  google/gemma-2                                            Gemma 2                                        Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.                                                             Google              \n",
      "142464  mehulbhattacharya/healthy-bleached-coral-reef-classifier  Coral Reef Health Image Classifier using YOLO  Detect Bleached or Healthy Coral Reef                                                                                                                                                                                 Mehul Bhattacharya  \n",
      "147213  darraghdog/qwen-qwen2.5-math-7b-instruct                  Qwen-Qwen2.5-Math-7B-Instruct                                                                                                                                                                                                                                        Darragh             \n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.model_list_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                                                deadline             category                reward  teamCount  userHasEntered  \n",
      "---------------------------------------------------------------------------------  -------------------  ---------------  -------------  ---------  --------------  \n",
      "https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2      2025-04-01 23:59:00  Featured         2,117,152 Usd        412           False  \n",
      "https://www.kaggle.com/competitions/gemma-language-tuning                          2025-01-15 00:59:00  Analytics          150,000 Usd          0           False  \n",
      "https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting  2025-01-13 23:59:00  Featured           120,000 Usd       1327           False  \n",
      "https://www.kaggle.com/competitions/gemini-long-context                            2024-12-01 23:59:00  Analytics          100,000 Usd          0           False  \n",
      "https://www.kaggle.com/competitions/nfl-big-data-bowl-2025                         2025-01-08 23:59:00  Analytics          100,000 Usd          0           False  \n",
      "https://www.kaggle.com/competitions/czii-cryo-et-object-identification             2025-02-05 23:59:00  Featured            75,000 Usd         52           False  \n",
      "https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use  2024-12-19 23:59:00  Featured            60,000 Usd       2183           False  \n",
      "https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics      2024-12-12 23:59:00  Featured            55,000 Usd       1101           False  \n",
      "https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants      2024-12-02 23:59:00  Research            50,000 Usd       1319           False  \n",
      "https://www.kaggle.com/competitions/playground-series-s4e11                        2024-11-30 23:59:00  Playground                Swag       1054           False  \n",
      "https://www.kaggle.com/competitions/titanic                                        2030-01-01 00:00:00  Getting Started      Knowledge      16172           False  \n",
      "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started      Knowledge       5590           False  \n",
      "https://www.kaggle.com/competitions/spaceship-titanic                              2030-01-01 00:00:00  Getting Started      Knowledge       1700           False  \n",
      "https://www.kaggle.com/competitions/digit-recognizer                               2030-01-01 00:00:00  Getting Started      Knowledge       1568           False  \n",
      "https://www.kaggle.com/competitions/nlp-getting-started                            2030-01-01 00:00:00  Getting Started      Knowledge       1012           False  \n",
      "https://www.kaggle.com/competitions/store-sales-time-series-forecasting            2030-06-30 23:59:00  Getting Started      Knowledge        776           False  \n",
      "https://www.kaggle.com/competitions/connectx                                       2030-01-01 00:00:00  Getting Started      Knowledge        203           False  \n",
      "https://www.kaggle.com/competitions/gan-getting-started                            2030-07-01 23:59:00  Getting Started      Knowledge         87           False  \n",
      "https://www.kaggle.com/competitions/contradictory-my-dear-watson                   2030-07-01 23:59:00  Getting Started      Knowledge         44           False  \n",
      "https://www.kaggle.com/competitions/tpu-getting-started                            2030-06-03 23:59:00  Getting Started      Knowledge         43           False  \n"
     ]
    }
   ],
   "source": [
    "! kaggle competitions list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can download datasets by using following command\n",
    "~~~\n",
    "! kaggle datasets download <name-of-dataset>\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/noxmoon/chinese-official-daily-news-since-2016\n",
      "License(s): unknown\n",
      "Downloading chinese-official-daily-news-since-2016.zip to c:\\Users\\wangs\\Dropbox\\個人用\\授業\\博士課程\\人文_2024_3Q\\DH\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/8.43M [00:00<?, ?B/s]\n",
      " 12%|█▏        | 1.00M/8.43M [00:00<00:05, 1.34MB/s]\n",
      " 24%|██▎       | 2.00M/8.43M [00:00<00:02, 2.42MB/s]\n",
      " 36%|███▌      | 3.00M/8.43M [00:01<00:01, 3.05MB/s]\n",
      " 47%|████▋     | 4.00M/8.43M [00:02<00:02, 1.68MB/s]\n",
      " 59%|█████▉    | 5.00M/8.43M [00:02<00:01, 1.93MB/s]\n",
      " 71%|███████   | 6.00M/8.43M [00:03<00:01, 2.09MB/s]\n",
      " 83%|████████▎ | 7.00M/8.43M [00:03<00:00, 2.19MB/s]\n",
      " 95%|█████████▍| 8.00M/8.43M [00:04<00:00, 2.18MB/s]\n",
      "100%|██████████| 8.43M/8.43M [00:04<00:00, 2.29MB/s]\n",
      "100%|██████████| 8.43M/8.43M [00:04<00:00, 2.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download noxmoon/chinese-official-daily-news-since-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese-official-daily-news-since-2016.zipを./extracted_dataに解凍しました。\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# ZIPファイルのパスを指定\n",
    "zip_file_path = \"chinese-official-daily-news-since-2016.zip\"\n",
    "extract_path = \"./extracted_data\"  # 解凍先ディレクトリを指定\n",
    "\n",
    "# 解凍先のディレクトリが存在しない場合は作成\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# ZIPファイルを解凍\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"{zip_file_path}を{extract_path}に解凍しました。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part2 xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tag</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>详细全文</td>\n",
       "      <td>陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...</td>\n",
       "      <td>中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>详细全文</td>\n",
       "      <td>中央军委印发《关于深化国防和军队改革的意见》</td>\n",
       "      <td>经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>详细全文</td>\n",
       "      <td>《习近平关于严明党的纪律和规矩论述摘编》出版发行</td>\n",
       "      <td>由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>详细全文</td>\n",
       "      <td>以实际行动向党中央看齐 向高标准努力</td>\n",
       "      <td>广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>详细全文</td>\n",
       "      <td>【年终特稿】关键之年 改革挺进深水区</td>\n",
       "      <td>刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   tag                                           headline  \\\n",
       "0  2016-01-01  详细全文  陆军领导机构火箭军战略支援部队成立大会在京举行 习近平向中国人民解放军陆军火箭军战略支援部队...   \n",
       "1  2016-01-01  详细全文                             中央军委印发《关于深化国防和军队改革的意见》   \n",
       "2  2016-01-01  详细全文                           《习近平关于严明党的纪律和规矩论述摘编》出版发行   \n",
       "3  2016-01-01  详细全文                                 以实际行动向党中央看齐 向高标准努力   \n",
       "4  2016-01-01  详细全文                                 【年终特稿】关键之年 改革挺进深水区   \n",
       "\n",
       "                                             content  \n",
       "0  中国人民解放军陆军领导机构、中国人民解放军火箭军、中国人民解放军战略支援部队成立大会2015...  \n",
       "1  经中央军委主席习近平批准，中央军委近日印发了《关于深化国防和军队改革的意见》。\\n《意见》强...  \n",
       "2  由中共中央纪律检查委员会、中共中央文献研究室编辑的《习近平关于严明党的纪律和规矩论述摘编》一...  \n",
       "3  广大党员干部正在积极学习习近平总书记在中央政治局专题民主生活会上的重要讲话。大家纷纷表示要把...  \n",
       "4  刚刚过去的2015年，是全面深化改革的关键之年。改革集中发力在制约经济社会发展的深层次矛盾，...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSVファイルの読み込み\n",
    "file_path = 'extracted_data/chinese_news.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 最初の数行を確認\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データには以下の列があります：\n",
    "\n",
    "・date: ニュースの日付\n",
    "tag: ニュースのタグ\n",
    "headline: ニュースの見出し\n",
    "content: ニュースの内容（この列を主に使用）\n",
    "\n",
    "次に、content列を使用してドキュメントのベクトル化とインデックス化を行い、RAGモデルの検索エンジン部分を作成します。faissを使ってインデックス化し、transformersを使ってテキストをベクトル化します。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルインデックスが正常に作成されました。\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# モデルとトークナイザーの準備（Sentence Transformersを使用）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ベクトル化の関数\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "# サンプルデータの取得\n",
    "sample_data = data['content'].sample(10, random_state=1)  # ランダムに10件を抽出\n",
    "\n",
    "# サンプルデータでベクトル化とインデックス化を試行\n",
    "try:\n",
    "    sample_embeddings = np.array([embed_text(text) for text in sample_data])\n",
    "    sample_index = faiss.IndexFlatL2(sample_embeddings.shape[1])\n",
    "    sample_index.add(sample_embeddings)\n",
    "    print(\"サンプルインデックスが正常に作成されました。\")\n",
    "except Exception as e:\n",
    "    print(\"エラーが発生しました:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "サンプルインデックスが正常に作成されました。\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# モデルとトークナイザーの準備（Sentence Transformersを使用）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ベクトル化の関数\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "# サンプルデータの取得\n",
    "sample_data = data['content'].sample(10, random_state=1)  # ランダムに10件を抽出\n",
    "\n",
    "# サンプルデータでベクトル化とインデックス化を試行\n",
    "try:\n",
    "    sample_embeddings = np.array([embed_text(text) for text in sample_data])\n",
    "    sample_index = faiss.IndexFlatL2(sample_embeddings.shape[1])\n",
    "    sample_index.add(sample_embeddings)\n",
    "    print(\"サンプルインデックスが正常に作成されました。\")\n",
    "except Exception as e:\n",
    "    print(\"エラーが発生しました:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=3):\n",
    "    # 質問をベクトル化\n",
    "    query_vector = embed_text(query).reshape(1, -1)\n",
    "    distances, indices = sample_index.search(query_vector, k)  # kは取得するドキュメント数\n",
    "    return [sample_data.iloc[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"\"\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    prompt = f\"以下のニュース情報に基づいて質問に答えてください。\\n\\n{context}\\n\\n質問: {query}\\n答え:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=15000\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成された答え: 2016年1月1日のニュースによれば、北京市は「十三五」规划を発表し、2020年までに北京城六区の常住人口を2014年の基準から約15%減少させることを目指しています。この対策には非首都機能の調整と人口の最適化が含まれています。\n",
      "\n",
      "また、2015年末までに全国で783万套の保障性安居工程が開工し、772万套が基本的に完成しました。これは2015年度の目標を上回る達成です。特に棚戸区改造に関しては601万套が開工し、年度\n"
     ]
    }
   ],
   "source": [
    "# 質問を設定し、検索と生成を行う\n",
    "query = \"2016-01-01のニュースをまとめてください\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "answer = generate_answer(query, retrieved_docs)\n",
    "\n",
    "print(\"生成された答え:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成された答え: 提供されたニュース情報には「关于深化国防和军队改革的意见」に関する情報は含まれていません。このため、具体的な日付をお答えすることができません。関連する情報を提供していただければ、お手伝いできるかもしれません。\n"
     ]
    }
   ],
   "source": [
    "# 質問を設定し、検索と生成を行う\n",
    "query = \"《关于深化国防和军队改革的意见》はいつのニュースですか？\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "answer = generate_answer(query, retrieved_docs)\n",
    "\n",
    "print(\"生成された答え:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
