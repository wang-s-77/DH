{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openai in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.54.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: certifi in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: torch in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: torch in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
    "! pip install pandas\n",
    "! pip install openai faiss-cpu transformers\n",
    "! pip install faiss-cpu\n",
    "! pip install torch\n",
    "! pip install numpy\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU langchain langchain-community langchain_text_splitters langchain_chroma\n",
    "! pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part1. Load from the cloud on the Kaggle database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle is the famous platform which is offering machine learning compititions and opensource datasets\n",
    "\n",
    "Note that if you want to use Kaggle, you must have an account but one can easily log in using one's Google or Fakebook or Yahoo account\n",
    "\n",
    "We need to install Kaggle API (Application Programming Interface) first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Collecting certifi>=2023.7.22 (from kaggle)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from kaggle) (2.9.0)\n",
      "Collecting requests (from kaggle)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm (from kaggle)\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting urllib3 (from kaggle)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting bleach (from kaggle)\n",
      "  Downloading bleach-6.2.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting webencodings (from bleach->kaggle)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->kaggle)\n",
      "  Downloading charset_normalizer-3.4.0-cp39-cp39-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->kaggle)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\wangs\\anaconda3\\envs\\dh\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading bleach-6.2.0-py3-none-any.whl (163 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp39-cp39-win_amd64.whl (102 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105797 sha256=806d80a5e7a85bff173dd323dd57722f7b423ad2895e93f82d03411925a4c855\n",
      "  Stored in directory: c:\\users\\wangs\\appdata\\local\\pip\\cache\\wheels\\2b\\af\\a9\\70bffa2773af622d2ebea9c8d407720b86e67bd40c465bf837\n",
      "Successfully built kaggle\n",
      "Installing collected packages: webencodings, text-unidecode, urllib3, tqdm, python-slugify, idna, charset-normalizer, certifi, bleach, requests, kaggle\n",
      "Successfully installed bleach-6.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 kaggle-1.6.17 python-slugify-8.0.4 requests-2.32.3 text-unidecode-1.3 tqdm-4.67.0 urllib3-2.2.3 webencodings-0.5.1\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the Kaggle home page in your browser, click on the icon in the upper right corner and select \"Account\" from the menu bar to go to the account management page. In the \"API\" section, select \"Create New API Token\" to download the kaggle.json file to your computer. **After uploading that json file to colab by clicking on the folder symbol in the leftmost tab**, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«APIãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¥ã‚Œã¦ãã ã•ã„:C:/Users/wangs/.kaggle\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/wangs/.kaggle\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "print(f\"æŒ‡å®šã—ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«APIãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…¥ã‚Œã¦ãã ã•ã„:{path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Page Token = CfDJ8GrUjsAKvhxNhvm67MmtKLl39PI19c56I5tY-a_zC4a4Hv2AGugR0W3XR5n4ElQhQ8XVOEzjSsXWZpCTF1zKo5Q\n",
      "    id  ref                                                       title                                          subtitle                                                                                                                                                                                                              author              \n",
      "------  --------------------------------------------------------  ---------------------------------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------  \n",
      "121027  metaresearch/llama-3.2                                    Llama 3.2                                      The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).                               Meta                \n",
      "145528  cohereforai/aya-expanse                                   Aya Expanse                                    Aya Expanse advances the state-of-the-art in multilingual capabilities to help bridge language gaps with AI.                                                                                                          CohereForAI         \n",
      "127417  abdullahmeda/qwen2.5                                      Qwen2.5                                                                                                                                                                                                                                                              Abdullah Meda       \n",
      "146350  dasbro/xgb_model                                          XGB_model                                                                                                                                                                                                                                                            dasbro              \n",
      "121954  google/gemma-2-2b-jpn-it                                  Gemma 2 2b JPN IT                              Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.                                                             Google              \n",
      "141350  ultralytics/yolo11                                        Ultralytics YOLO11                             Ultralytics YOLO ğŸš€ for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.                                                                                 Ultralytics         \n",
      "  3301  google/gemma                                              Gemma                                          Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.                                                                                    Google              \n",
      "123481  takanashihumbert/qwen2.5                                  Qwen2.5                                                                                                                                                                                                                                                              Joseph              \n",
      "121030  metaresearch/llama-3.2-vision                             Llama 3.2 Vision                               The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text + images in / text out).  Meta                \n",
      "  3533  keras/gemma                                               Gemma                                          Keras implementation of the Gemma model. This Keras 3 implementation will run on JAX, TensorFlow and PyTorch.                                                                                                         Keras               \n",
      "  1902  mistral-ai/mistral                                        Mistral                                        Mistral AI team is proud to release Mistral, the most powerful language model for its size to date.                                                                                                                   Mistral AI          \n",
      "141013  ultralytics/yolov8                                        Ultralytics YOLOv8                             Ultralytics YOLO ğŸš€ for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.                                                                                 Ultralytics         \n",
      "141044  huikang/qwen2.5                                           qwen2.5                                                                                                                                                                                                                                                              Tong Hui Kang       \n",
      " 91102  metaresearch/llama-3.1                                    Llama 3.1                                      The Meta Llama 3.1 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).                        Meta                \n",
      " 78150  keras/gemma2                                              Gemma 2                                        Keras implementation of the Gemma 2 model. This Keras 3 implementation will run on JAX, TensorFlow and PyTorch.                                                                                                       Keras               \n",
      "128845  jonathanchan/baai                                         BAAI                                                                                                                                                                                                                                                                 Jonathan Chan       \n",
      "152800  ai-singapore/gemma2-9b-cpt-sea-lionv3-instruct            gemma2-9b-cpt-sea-lionv3-instruct              Gemma2 9B CPT SEA-LIONv3 Instruct                                                                                                                                                                                     AI Singapore        \n",
      " 76277  google/gemma-2                                            Gemma 2                                        Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models.                                                             Google              \n",
      "142464  mehulbhattacharya/healthy-bleached-coral-reef-classifier  Coral Reef Health Image Classifier using YOLO  Detect Bleached or Healthy Coral Reef                                                                                                                                                                                 Mehul Bhattacharya  \n",
      "147213  darraghdog/qwen-qwen2.5-math-7b-instruct                  Qwen-Qwen2.5-Math-7B-Instruct                                                                                                                                                                                                                                        Darragh             \n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.model_list_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref                                                                                deadline             category                reward  teamCount  userHasEntered  \n",
      "---------------------------------------------------------------------------------  -------------------  ---------------  -------------  ---------  --------------  \n",
      "https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2      2025-04-01 23:59:00  Featured         2,117,152 Usd        412           False  \n",
      "https://www.kaggle.com/competitions/gemma-language-tuning                          2025-01-15 00:59:00  Analytics          150,000 Usd          0           False  \n",
      "https://www.kaggle.com/competitions/jane-street-real-time-market-data-forecasting  2025-01-13 23:59:00  Featured           120,000 Usd       1327           False  \n",
      "https://www.kaggle.com/competitions/gemini-long-context                            2024-12-01 23:59:00  Analytics          100,000 Usd          0           False  \n",
      "https://www.kaggle.com/competitions/nfl-big-data-bowl-2025                         2025-01-08 23:59:00  Analytics          100,000 Usd          0           False  \n",
      "https://www.kaggle.com/competitions/czii-cryo-et-object-identification             2025-02-05 23:59:00  Featured            75,000 Usd         52           False  \n",
      "https://www.kaggle.com/competitions/child-mind-institute-problematic-internet-use  2024-12-19 23:59:00  Featured            60,000 Usd       2183           False  \n",
      "https://www.kaggle.com/competitions/eedi-mining-misconceptions-in-mathematics      2024-12-12 23:59:00  Featured            55,000 Usd       1101           False  \n",
      "https://www.kaggle.com/competitions/um-game-playing-strength-of-mcts-variants      2024-12-02 23:59:00  Research            50,000 Usd       1319           False  \n",
      "https://www.kaggle.com/competitions/playground-series-s4e11                        2024-11-30 23:59:00  Playground                Swag       1054           False  \n",
      "https://www.kaggle.com/competitions/titanic                                        2030-01-01 00:00:00  Getting Started      Knowledge      16172           False  \n",
      "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started      Knowledge       5590           False  \n",
      "https://www.kaggle.com/competitions/spaceship-titanic                              2030-01-01 00:00:00  Getting Started      Knowledge       1700           False  \n",
      "https://www.kaggle.com/competitions/digit-recognizer                               2030-01-01 00:00:00  Getting Started      Knowledge       1568           False  \n",
      "https://www.kaggle.com/competitions/nlp-getting-started                            2030-01-01 00:00:00  Getting Started      Knowledge       1012           False  \n",
      "https://www.kaggle.com/competitions/store-sales-time-series-forecasting            2030-06-30 23:59:00  Getting Started      Knowledge        776           False  \n",
      "https://www.kaggle.com/competitions/connectx                                       2030-01-01 00:00:00  Getting Started      Knowledge        203           False  \n",
      "https://www.kaggle.com/competitions/gan-getting-started                            2030-07-01 23:59:00  Getting Started      Knowledge         87           False  \n",
      "https://www.kaggle.com/competitions/contradictory-my-dear-watson                   2030-07-01 23:59:00  Getting Started      Knowledge         44           False  \n",
      "https://www.kaggle.com/competitions/tpu-getting-started                            2030-06-03 23:59:00  Getting Started      Knowledge         43           False  \n"
     ]
    }
   ],
   "source": [
    "! kaggle competitions list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can download datasets by using following command\n",
    "~~~\n",
    "! kaggle datasets download <name-of-dataset>\n",
    "~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/noxmoon/chinese-official-daily-news-since-2016\n",
      "License(s): unknown\n",
      "Downloading chinese-official-daily-news-since-2016.zip to c:\\Users\\wangs\\Dropbox\\å€‹äººç”¨\\æˆæ¥­\\åšå£«èª²ç¨‹\\äººæ–‡_2024_3Q\\DH\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/8.43M [00:00<?, ?B/s]\n",
      " 12%|â–ˆâ–        | 1.00M/8.43M [00:00<00:05, 1.34MB/s]\n",
      " 24%|â–ˆâ–ˆâ–       | 2.00M/8.43M [00:00<00:02, 2.42MB/s]\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3.00M/8.43M [00:01<00:01, 3.05MB/s]\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4.00M/8.43M [00:02<00:02, 1.68MB/s]\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 5.00M/8.43M [00:02<00:01, 1.93MB/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 6.00M/8.43M [00:03<00:01, 2.09MB/s]\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 7.00M/8.43M [00:03<00:00, 2.19MB/s]\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 8.00M/8.43M [00:04<00:00, 2.18MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.43M/8.43M [00:04<00:00, 2.29MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.43M/8.43M [00:04<00:00, 2.13MB/s]\n"
     ]
    }
   ],
   "source": [
    "! kaggle datasets download noxmoon/chinese-official-daily-news-since-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese-official-daily-news-since-2016.zipã‚’./extracted_dataã«è§£å‡ã—ã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
    "zip_file_path = \"chinese-official-daily-news-since-2016.zip\"\n",
    "extract_path = \"./extracted_data\"  # è§£å‡å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š\n",
    "\n",
    "# è§£å‡å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£å‡\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"{zip_file_path}ã‚’{extract_path}ã«è§£å‡ã—ã¾ã—ãŸã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part2 xxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tag</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>è¯¦ç»†å…¨æ–‡</td>\n",
       "      <td>é™†å†›é¢†å¯¼æœºæ„ç«ç®­å†›æˆ˜ç•¥æ”¯æ´éƒ¨é˜Ÿæˆç«‹å¤§ä¼šåœ¨äº¬ä¸¾è¡Œ ä¹ è¿‘å¹³å‘ä¸­å›½äººæ°‘è§£æ”¾å†›é™†å†›ç«ç®­å†›æˆ˜ç•¥æ”¯æ´éƒ¨é˜Ÿ...</td>\n",
       "      <td>ä¸­å›½äººæ°‘è§£æ”¾å†›é™†å†›é¢†å¯¼æœºæ„ã€ä¸­å›½äººæ°‘è§£æ”¾å†›ç«ç®­å†›ã€ä¸­å›½äººæ°‘è§£æ”¾å†›æˆ˜ç•¥æ”¯æ´éƒ¨é˜Ÿæˆç«‹å¤§ä¼š2015...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>è¯¦ç»†å…¨æ–‡</td>\n",
       "      <td>ä¸­å¤®å†›å§”å°å‘ã€Šå…³äºæ·±åŒ–å›½é˜²å’Œå†›é˜Ÿæ”¹é©çš„æ„è§ã€‹</td>\n",
       "      <td>ç»ä¸­å¤®å†›å§”ä¸»å¸­ä¹ è¿‘å¹³æ‰¹å‡†ï¼Œä¸­å¤®å†›å§”è¿‘æ—¥å°å‘äº†ã€Šå…³äºæ·±åŒ–å›½é˜²å’Œå†›é˜Ÿæ”¹é©çš„æ„è§ã€‹ã€‚\\nã€Šæ„è§ã€‹å¼º...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>è¯¦ç»†å…¨æ–‡</td>\n",
       "      <td>ã€Šä¹ è¿‘å¹³å…³äºä¸¥æ˜å…šçš„çºªå¾‹å’Œè§„çŸ©è®ºè¿°æ‘˜ç¼–ã€‹å‡ºç‰ˆå‘è¡Œ</td>\n",
       "      <td>ç”±ä¸­å…±ä¸­å¤®çºªå¾‹æ£€æŸ¥å§”å‘˜ä¼šã€ä¸­å…±ä¸­å¤®æ–‡çŒ®ç ”ç©¶å®¤ç¼–è¾‘çš„ã€Šä¹ è¿‘å¹³å…³äºä¸¥æ˜å…šçš„çºªå¾‹å’Œè§„çŸ©è®ºè¿°æ‘˜ç¼–ã€‹ä¸€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>è¯¦ç»†å…¨æ–‡</td>\n",
       "      <td>ä»¥å®é™…è¡ŒåŠ¨å‘å…šä¸­å¤®çœ‹é½ å‘é«˜æ ‡å‡†åŠªåŠ›</td>\n",
       "      <td>å¹¿å¤§å…šå‘˜å¹²éƒ¨æ­£åœ¨ç§¯æå­¦ä¹ ä¹ è¿‘å¹³æ€»ä¹¦è®°åœ¨ä¸­å¤®æ”¿æ²»å±€ä¸“é¢˜æ°‘ä¸»ç”Ÿæ´»ä¼šä¸Šçš„é‡è¦è®²è¯ã€‚å¤§å®¶çº·çº·è¡¨ç¤ºè¦æŠŠ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>è¯¦ç»†å…¨æ–‡</td>\n",
       "      <td>ã€å¹´ç»ˆç‰¹ç¨¿ã€‘å…³é”®ä¹‹å¹´ æ”¹é©æŒºè¿›æ·±æ°´åŒº</td>\n",
       "      <td>åˆšåˆšè¿‡å»çš„2015å¹´ï¼Œæ˜¯å…¨é¢æ·±åŒ–æ”¹é©çš„å…³é”®ä¹‹å¹´ã€‚æ”¹é©é›†ä¸­å‘åŠ›åœ¨åˆ¶çº¦ç»æµç¤¾ä¼šå‘å±•çš„æ·±å±‚æ¬¡çŸ›ç›¾ï¼Œ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   tag                                           headline  \\\n",
       "0  2016-01-01  è¯¦ç»†å…¨æ–‡  é™†å†›é¢†å¯¼æœºæ„ç«ç®­å†›æˆ˜ç•¥æ”¯æ´éƒ¨é˜Ÿæˆç«‹å¤§ä¼šåœ¨äº¬ä¸¾è¡Œ ä¹ è¿‘å¹³å‘ä¸­å›½äººæ°‘è§£æ”¾å†›é™†å†›ç«ç®­å†›æˆ˜ç•¥æ”¯æ´éƒ¨é˜Ÿ...   \n",
       "1  2016-01-01  è¯¦ç»†å…¨æ–‡                             ä¸­å¤®å†›å§”å°å‘ã€Šå…³äºæ·±åŒ–å›½é˜²å’Œå†›é˜Ÿæ”¹é©çš„æ„è§ã€‹   \n",
       "2  2016-01-01  è¯¦ç»†å…¨æ–‡                           ã€Šä¹ è¿‘å¹³å…³äºä¸¥æ˜å…šçš„çºªå¾‹å’Œè§„çŸ©è®ºè¿°æ‘˜ç¼–ã€‹å‡ºç‰ˆå‘è¡Œ   \n",
       "3  2016-01-01  è¯¦ç»†å…¨æ–‡                                 ä»¥å®é™…è¡ŒåŠ¨å‘å…šä¸­å¤®çœ‹é½ å‘é«˜æ ‡å‡†åŠªåŠ›   \n",
       "4  2016-01-01  è¯¦ç»†å…¨æ–‡                                 ã€å¹´ç»ˆç‰¹ç¨¿ã€‘å…³é”®ä¹‹å¹´ æ”¹é©æŒºè¿›æ·±æ°´åŒº   \n",
       "\n",
       "                                             content  \n",
       "0  ä¸­å›½äººæ°‘è§£æ”¾å†›é™†å†›é¢†å¯¼æœºæ„ã€ä¸­å›½äººæ°‘è§£æ”¾å†›ç«ç®­å†›ã€ä¸­å›½äººæ°‘è§£æ”¾å†›æˆ˜ç•¥æ”¯æ´éƒ¨é˜Ÿæˆç«‹å¤§ä¼š2015...  \n",
       "1  ç»ä¸­å¤®å†›å§”ä¸»å¸­ä¹ è¿‘å¹³æ‰¹å‡†ï¼Œä¸­å¤®å†›å§”è¿‘æ—¥å°å‘äº†ã€Šå…³äºæ·±åŒ–å›½é˜²å’Œå†›é˜Ÿæ”¹é©çš„æ„è§ã€‹ã€‚\\nã€Šæ„è§ã€‹å¼º...  \n",
       "2  ç”±ä¸­å…±ä¸­å¤®çºªå¾‹æ£€æŸ¥å§”å‘˜ä¼šã€ä¸­å…±ä¸­å¤®æ–‡çŒ®ç ”ç©¶å®¤ç¼–è¾‘çš„ã€Šä¹ è¿‘å¹³å…³äºä¸¥æ˜å…šçš„çºªå¾‹å’Œè§„çŸ©è®ºè¿°æ‘˜ç¼–ã€‹ä¸€...  \n",
       "3  å¹¿å¤§å…šå‘˜å¹²éƒ¨æ­£åœ¨ç§¯æå­¦ä¹ ä¹ è¿‘å¹³æ€»ä¹¦è®°åœ¨ä¸­å¤®æ”¿æ²»å±€ä¸“é¢˜æ°‘ä¸»ç”Ÿæ´»ä¼šä¸Šçš„é‡è¦è®²è¯ã€‚å¤§å®¶çº·çº·è¡¨ç¤ºè¦æŠŠ...  \n",
       "4  åˆšåˆšè¿‡å»çš„2015å¹´ï¼Œæ˜¯å…¨é¢æ·±åŒ–æ”¹é©çš„å…³é”®ä¹‹å¹´ã€‚æ”¹é©é›†ä¸­å‘åŠ›åœ¨åˆ¶çº¦ç»æµç¤¾ä¼šå‘å±•çš„æ·±å±‚æ¬¡çŸ›ç›¾ï¼Œ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿\n",
    "file_path = 'extracted_data/chinese_news.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# æœ€åˆã®æ•°è¡Œã‚’ç¢ºèª\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒ‡ãƒ¼ã‚¿ã«ã¯ä»¥ä¸‹ã®åˆ—ãŒã‚ã‚Šã¾ã™ï¼š\n",
    "\n",
    "ãƒ»date: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æ—¥ä»˜\n",
    "tag: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚°\n",
    "headline: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦‹å‡ºã—\n",
    "content: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å†…å®¹ï¼ˆã“ã®åˆ—ã‚’ä¸»ã«ä½¿ç”¨ï¼‰\n",
    "\n",
    "æ¬¡ã«ã€contentåˆ—ã‚’ä½¿ç”¨ã—ã¦ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã‚’è¡Œã„ã€RAGãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³éƒ¨åˆ†ã‚’ä½œæˆã—ã¾ã™ã€‚faissã‚’ä½¿ã£ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã—ã€transformersã‚’ä½¿ã£ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¾ã™ã€‚ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™ï¼ˆSentence Transformersã‚’ä½¿ç”¨ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®é–¢æ•°\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—\n",
    "sample_data = data['content'].sample(10, random_state=1)  # ãƒ©ãƒ³ãƒ€ãƒ ã«10ä»¶ã‚’æŠ½å‡º\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã‚’è©¦è¡Œ\n",
    "try:\n",
    "    sample_embeddings = np.array([embed_text(text) for text in sample_data])\n",
    "    sample_index = faiss.IndexFlatL2(sample_embeddings.shape[1])\n",
    "    sample_index.add(sample_embeddings)\n",
    "    print(\"ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "except Exception as e:\n",
    "    print(\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™ï¼ˆSentence Transformersã‚’ä½¿ç”¨ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®é–¢æ•°\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].numpy()\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—\n",
    "sample_data = data['content'].sample(10, random_state=1)  # ãƒ©ãƒ³ãƒ€ãƒ ã«10ä»¶ã‚’æŠ½å‡º\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã‚’è©¦è¡Œ\n",
    "try:\n",
    "    sample_embeddings = np.array([embed_text(text) for text in sample_data])\n",
    "    sample_index = faiss.IndexFlatL2(sample_embeddings.shape[1])\n",
    "    sample_index.add(sample_embeddings)\n",
    "    print(\"ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã¾ã—ãŸã€‚\")\n",
    "except Exception as e:\n",
    "    print(\"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=3):\n",
    "    # è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "    query_vector = embed_text(query).reshape(1, -1)\n",
    "    distances, indices = sample_index.search(query_vector, k)  # kã¯å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°\n",
    "    return [sample_data.iloc[i] for i in indices[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"\"\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    prompt = f\"ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚\\n\\n{context}\\n\\nè³ªå•: {query}\\nç­”ãˆ:\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=15000\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆã•ã‚ŒãŸç­”ãˆ: 2016å¹´1æœˆ1æ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«ã‚ˆã‚Œã°ã€åŒ—äº¬å¸‚ã¯ã€Œåä¸‰äº”ã€è§„åˆ’ã‚’ç™ºè¡¨ã—ã€2020å¹´ã¾ã§ã«åŒ—äº¬åŸå…­åŒºã®å¸¸ä½äººå£ã‚’2014å¹´ã®åŸºæº–ã‹ã‚‰ç´„15%æ¸›å°‘ã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚ã“ã®å¯¾ç­–ã«ã¯éé¦–éƒ½æ©Ÿèƒ½ã®èª¿æ•´ã¨äººå£ã®æœ€é©åŒ–ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
      "\n",
      "ã¾ãŸã€2015å¹´æœ«ã¾ã§ã«å…¨å›½ã§783ä¸‡å¥—ã®ä¿éšœæ€§å®‰å±…å·¥ç¨‹ãŒé–‹å·¥ã—ã€772ä¸‡å¥—ãŒåŸºæœ¬çš„ã«å®Œæˆã—ã¾ã—ãŸã€‚ã“ã‚Œã¯2015å¹´åº¦ã®ç›®æ¨™ã‚’ä¸Šå›ã‚‹é”æˆã§ã™ã€‚ç‰¹ã«æ£šæˆ¸åŒºæ”¹é€ ã«é–¢ã—ã¦ã¯601ä¸‡å¥—ãŒé–‹å·¥ã—ã€å¹´åº¦\n"
     ]
    }
   ],
   "source": [
    "# è³ªå•ã‚’è¨­å®šã—ã€æ¤œç´¢ã¨ç”Ÿæˆã‚’è¡Œã†\n",
    "query = \"2016-01-01ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "answer = generate_answer(query, retrieved_docs)\n",
    "\n",
    "print(\"ç”Ÿæˆã•ã‚ŒãŸç­”ãˆ:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆã•ã‚ŒãŸç­”ãˆ: æä¾›ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹æƒ…å ±ã«ã¯ã€Œå…³äºæ·±åŒ–å›½é˜²å’Œå†›é˜Ÿæ”¹é©çš„æ„è§ã€ã«é–¢ã™ã‚‹æƒ…å ±ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã“ã®ãŸã‚ã€å…·ä½“çš„ãªæ—¥ä»˜ã‚’ãŠç­”ãˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ã‚’æä¾›ã—ã¦ã„ãŸã ã‘ã‚Œã°ã€ãŠæ‰‹ä¼ã„ã§ãã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è³ªå•ã‚’è¨­å®šã—ã€æ¤œç´¢ã¨ç”Ÿæˆã‚’è¡Œã†\n",
    "query = \"ã€Šå…³äºæ·±åŒ–å›½é˜²å’Œå†›é˜Ÿæ”¹é©çš„æ„è§ã€‹ã¯ã„ã¤ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã‹ï¼Ÿ\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "answer = generate_answer(query, retrieved_docs)\n",
    "\n",
    "print(\"ç”Ÿæˆã•ã‚ŒãŸç­”ãˆ:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
